''' 
Initial code Adapted from Langchain tutorial
(https://python.langchain.com/docs/tutorials/rag/)
and generated by ChatGPT model GPT-4o
(https://chatgpt.com/share/67def591-3128-8004-baa0-64fe0d1630df,
https://chatgpt.com/share/683083e5-6898-8010-9b0e-be936819f6ac)
The generated code and snippets from LangChain are heavily modified by the author to fit the needs of the project.
'''
import os
import re
import json
from backend import constants # Import API keys
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chat_models import init_chat_model

# Set environment variables for API keys
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGSMITH_API_KEY"] = constants.LANGSMITH_API_KEY
os.environ["LANGSMITH_PROJECT"] = "" # Not set in the public release.
os.environ["OPENAI_API_KEY"] = constants.OpenAI_APIKEY

# Initialize the large language model and embedding model.
# Added temperature and max_tokens parameters for project-specific configuration.
llm = init_chat_model("gpt-4o-mini", model_provider="openai", temperature=0.3, max_tokens=250)
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# Set the path to the chunked document.
Chunked_Act = "Recursive_Chunks_sw.json" 

# Load JSON file and extract chunks.
with open(Chunked_Act, "r", encoding="utf-8") as file:
    data = json.load(file)
    chunks = data

# Create Document objects from the chunks.
docs = [Document(page_content=chunk["text"], metadata={"id": chunk["id"], "page": chunk["page"]}) for chunk in chunks]

# Create FAISS vector store.
faiss_index = "faiss_index_sw"
if os.path.exists(faiss_index):
    vector_store = FAISS.load_local(faiss_index, embeddings, allow_dangerous_deserialization=True)
    print("Loaded FAISS index from disk.")
else:
    vector_store = FAISS.from_documents(docs, embeddings)
    vector_store.save_local(faiss_index)
    print("Created and saved FAISS index.")

# Define a prompt template using Langchain's PromptTemplate class.
prompt = PromptTemplate.from_template(
    "Context: {context}\n\nQuestion: {question}\n\nAnswer:"
)

# Define state for application
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str

# Define application steps
def retrieve(state: State):
    '''
    This function retrieves relevant information (chunks) based on the user's query.
    It uses a FAISS vector store to find the most similar chunks to the query.
    It also filters the retrieved chunks based on their scores (L2 distance).

    Args:
        State (class): The state class containing the question and context.

    Returns:
        Dict: a dictionary containing the retrieved and filtered chunks.
    '''
    # Specify the number of chunks to originally retrieve.
    Nchunks_search = 4
    # Specify the maximum number of chunks to use for context.
    Nchunks_filter = 2

    # Retrieve chunks with scores.
    retrieved_docs = vector_store.similarity_search_with_score(state["question"], k=Nchunks_search)
    
    # Get the best scored chunk (lowest distance).
    best_score = retrieved_docs[0][1]

    # Define a filtration system using best-scoring method.
    if best_score > 1.4:
        print(f"Best score is {best_score}, no relevant information found.")
        return {"context": [{"text": "Ingen relevant informasjon funnet i dokumentet.", "id": "no id", "page": "No Page"}]}
    
    # Define a threshold: only allow chunks that are within 40% of the best score
    threshold = best_score * 1.4
    
    # Filter out chunks that are worse than the threshold
    filtered_docs = [(doc, score) for doc, score in retrieved_docs if score <= threshold][:Nchunks_filter]
    
    # Print the filtered chunks and their scores. This is for debugging purposes
    for doc, score in filtered_docs:
        print(f"ID: {doc.metadata.get('id', 'No ID')}, Page: {doc.metadata.get('page', 'No Page')}, score: {score}")
    
    # Return the context with the filtered documents
    return {"context": [{"text": doc.page_content, "id": doc.metadata.get("id", "No ID")} for doc, _ in filtered_docs]}

    
def generate(state: State):
    '''
    This function sends an API request to a LLM to generate an answer based on the question and context provided.
    It uses a prompt template to format the input for the LLM.

    Args:
        State (class): The state class containing the question and context.

    Returns:
        Str: containing the API response with the generated answer.
    
    '''
    docs_content = "\n\n".join(doc["text"] for doc in state["context"])

    # Use prompt.format to fill in the template with the question and context.
    formatted_prompt = prompt.format(question=state["question"], context=docs_content)
    messages = [
        {"role": "system", "content": 
            "Du er en profesjonell og presis AI-assistent som er ekspert i jus og EU-regelverk angående KI-forordningen."
            "Du har tilgang til et dokument som inneholder informasjon om KI-forordningen "
            "Svar på spørsmålet basert på informasjonen i dokumentet."
            "Vennligst svar med maks 100 ord."
            "Svar på en tydelig og profesjonell måte, uavhengig av dokumentets struktur."
            "Hvis dokumentet inneholder stikkord eller ufullstendige setninger, omskriv svaret ditt til en sammenhengende og velstrukturert tekst."
            "Hvis spørsmålet er om et annet emne enn KI-forordningen, svar kort og høflig om at du kun kan svare på spørsmål relatert til KI-forordningen."
            "Hvis det gitte dokumentet ikke inneholder relevant informasjon, svar kort at KI-forordningen sannsynligvis dekker ikke eksplisitt dette emnet."
            },
        {"role": "user", "content": formatted_prompt}
    ]
    
    response = llm.invoke(messages) # Uncomment this line to get the actual response
    return {"answer": response.content} # Uncomment this line to get the actual response
    #return {"answer": "test"} # Placeholder for actual response. This is for testing server response without the LLM, to avoid unneccesary API usage. re-comment after testing

# Connect the RAG process into a sequence
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()


#This part of the code is for the suggested questions feature.

# Load the suggested questions from a text file
suggested_questions = "common_questions.txt"
with open(suggested_questions, "r", encoding="utf-8") as file:
    suggestions = [line.strip() for line in file.readlines()]

# Load the FAISS vector store for suggestions, or create it if it doesn't exist
faiss_suggestion_index = "faiss_suggestions"
if os.path.exists(faiss_suggestion_index):
    vector_store_suggestions = FAISS.load_local(faiss_suggestion_index, embeddings, allow_dangerous_deserialization=True)
    print("Loaded FAISS index for suggestions.")
else:
    docs = [Document(page_content=suggestion) for suggestion in suggestions]
    vector_store_suggestions = FAISS.from_documents(docs, embeddings)
    vector_store_suggestions.save_local(faiss_suggestion_index)
    print("Created and saved FAISS index for suggestions.")

def suggest_similar_questions(query: str, top_k: int = 3):
    '''
    This function retrieves similar questions based on the user's query
    It uses a FAISS vector store to find the most similar questions
    
    Args:
        query (str): The user's query
        top_k (int): The number of similar questions to retrieve

    Returns:
        List[str]: A list of similar questions
    '''

    # Check if the query is empty, if so return an empty list. The function should not intentially be called on an empty query, but this is a safeguard.
    if not query.strip():
        return []
    results = vector_store_suggestions.similarity_search(query, k=top_k)
    return [result.page_content for result in results]


r'''
This part of the code is for testing the RAG process, without the need for the server or web-UI.

query = str(input("Enter your query: "))
response = graph.invoke({"question": query})
cleaned_answer = re.sub(r"\*\*", r"", response["answer"])
print(response["answer"])
print(f'Answer: {cleaned_answer}')
'''
